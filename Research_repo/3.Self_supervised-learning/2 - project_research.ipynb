{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40309098",
   "metadata": {},
   "source": [
    "## Docs\n",
    "\n",
    "* https://forums.fast.ai/t/porto-seguro-winning-solution-representation-learning/8499 winning solution porto seguro\n",
    "* https://github.com/BenjiKCF/Tabular-data-Winning-Solution tabular solution to porto seguro\n",
    "* To do list\n",
    "    * To compare other variations of embeddings like using the class information for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec1df76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.146081</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119859</td>\n",
       "      <td>0.117541</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.114867</td>\n",
       "      <td>0.114758</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113110</td>\n",
       "      <td>0.113391</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110490</td>\n",
       "      <td>0.112978</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.109056</td>\n",
       "      <td>0.111802</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107009</td>\n",
       "      <td>0.110832</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.105479</td>\n",
       "      <td>0.110451</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cat0_0</th>\n",
       "      <th>cat0_1</th>\n",
       "      <th>cat0_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277491</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292775</td>\n",
       "      <td>0.210421</td>\n",
       "      <td>0.335680</td>\n",
       "      <td>0.508110</td>\n",
       "      <td>0.375804</td>\n",
       "      <td>0.485737</td>\n",
       "      <td>0.388535</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59826</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717059</td>\n",
       "      <td>0.555945</td>\n",
       "      <td>0.253756</td>\n",
       "      <td>0.810781</td>\n",
       "      <td>0.558822</td>\n",
       "      <td>0.821682</td>\n",
       "      <td>0.950494</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100532</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300573</td>\n",
       "      <td>0.807069</td>\n",
       "      <td>0.559432</td>\n",
       "      <td>0.310892</td>\n",
       "      <td>0.414348</td>\n",
       "      <td>0.328559</td>\n",
       "      <td>0.198210</td>\n",
       "      <td>-0.055040</td>\n",
       "      <td>-0.069424</td>\n",
       "      <td>-0.050039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127444</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260020</td>\n",
       "      <td>0.762579</td>\n",
       "      <td>0.929469</td>\n",
       "      <td>0.368868</td>\n",
       "      <td>0.400274</td>\n",
       "      <td>0.582639</td>\n",
       "      <td>0.436348</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268466</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.487171</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.332349</td>\n",
       "      <td>0.366024</td>\n",
       "      <td>0.374952</td>\n",
       "      <td>0.382322</td>\n",
       "      <td>-0.055040</td>\n",
       "      <td>-0.069424</td>\n",
       "      <td>-0.050039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292950</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310330</td>\n",
       "      <td>0.354413</td>\n",
       "      <td>0.335381</td>\n",
       "      <td>0.433126</td>\n",
       "      <td>0.758386</td>\n",
       "      <td>0.438863</td>\n",
       "      <td>0.488459</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235306</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731341</td>\n",
       "      <td>0.734535</td>\n",
       "      <td>0.548252</td>\n",
       "      <td>0.304629</td>\n",
       "      <td>0.338420</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.339444</td>\n",
       "      <td>-0.055040</td>\n",
       "      <td>-0.069424</td>\n",
       "      <td>-0.050039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187628</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278301</td>\n",
       "      <td>0.735457</td>\n",
       "      <td>0.787859</td>\n",
       "      <td>0.313780</td>\n",
       "      <td>0.700493</td>\n",
       "      <td>0.799430</td>\n",
       "      <td>0.464530</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200243</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247176</td>\n",
       "      <td>0.052474</td>\n",
       "      <td>0.172384</td>\n",
       "      <td>0.552959</td>\n",
       "      <td>0.405473</td>\n",
       "      <td>0.266106</td>\n",
       "      <td>0.531018</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155878</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305348</td>\n",
       "      <td>0.680329</td>\n",
       "      <td>0.534171</td>\n",
       "      <td>0.381683</td>\n",
       "      <td>0.435032</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.263918</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  ...  \\\n",
       "277491     6     4     2     7    34     1    31    60     1    148  ...   \n",
       "59826     11     1     1    13    34     3    20     4    13    258  ...   \n",
       "100532     6    10     1     5    34     3     1    36     1     76  ...   \n",
       "127444     8    13     1     6     3     1     9    60     1    148  ...   \n",
       "268466    13     1     2     5    34     1     9    52     5    160  ...   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   \n",
       "292950     8    13     1     6     3     5    47    51     1    175  ...   \n",
       "235306     6     3     1     5    34     3    20    11     1     55  ...   \n",
       "187628    15     3     1     6    34     1    37    34     1    148  ...   \n",
       "200243     6     1     2     5     3     1    10    47     5    175  ...   \n",
       "155878     6     4     1     6    34     3    23    37     1    119  ...   \n",
       "\n",
       "           cont4     cont5     cont6     cont7     cont8     cont9    cont10  \\\n",
       "277491  0.292775  0.210421  0.335680  0.508110  0.375804  0.485737  0.388535   \n",
       "59826   0.717059  0.555945  0.253756  0.810781  0.558822  0.821682  0.950494   \n",
       "100532  0.300573  0.807069  0.559432  0.310892  0.414348  0.328559  0.198210   \n",
       "127444  0.260020  0.762579  0.929469  0.368868  0.400274  0.582639  0.436348   \n",
       "268466  0.292645  0.487171  0.581556  0.332349  0.366024  0.374952  0.382322   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "292950  0.310330  0.354413  0.335381  0.433126  0.758386  0.438863  0.488459   \n",
       "235306  0.731341  0.734535  0.548252  0.304629  0.338420  0.321991  0.339444   \n",
       "187628  0.278301  0.735457  0.787859  0.313780  0.700493  0.799430  0.464530   \n",
       "200243  0.247176  0.052474  0.172384  0.552959  0.405473  0.266106  0.531018   \n",
       "155878  0.305348  0.680329  0.534171  0.381683  0.435032  0.490791  0.263918   \n",
       "\n",
       "          cat0_0    cat0_1    cat0_2  \n",
       "277491  0.055335  0.035863  0.053796  \n",
       "59826   0.055335  0.035863  0.053796  \n",
       "100532 -0.055040 -0.069424 -0.050039  \n",
       "127444  0.055335  0.035863  0.053796  \n",
       "268466 -0.055040 -0.069424 -0.050039  \n",
       "...          ...       ...       ...  \n",
       "292950  0.055335  0.035863  0.053796  \n",
       "235306 -0.055040 -0.069424 -0.050039  \n",
       "187628  0.055335  0.035863  0.053796  \n",
       "200243  0.055335  0.035863  0.053796  \n",
       "155878  0.055335  0.035863  0.053796  \n",
       "\n",
       "[60000 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.tabular.all import *\n",
    "from fastcore.utils import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_nn = pd.read_csv('dataset/train.csv', low_memory=False)\n",
    "df_nn_final = df_nn.drop('id', axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Categorical embedding\n",
    "\"\"\"\n",
    "\n",
    "cont,cat = cont_cat_split(df_nn_final, max_card=9000, dep_var='target')\n",
    "procs_nn = [Categorify, Normalize]\n",
    "splits = RandomSplitter(seed=23)(df_nn_final)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "to_nn = TabularPandas(df_nn, procs_nn, cat, cont,\n",
    "                      splits=splits, y_names='target')\n",
    "dls = to_nn.dataloaders(1024, device = device)\n",
    "\n",
    "learn = tabular_learner(dls, layers=[500,250], n_out=1)\n",
    "learn.fit_one_cycle(8, 5e-4)\n",
    "\n",
    "preds,targs = learn.get_preds()\n",
    "roc_auc_score(targs, preds)\n",
    "\n",
    "learn.save('learn8')\n",
    "\n",
    "# Machine Learning Models\n",
    "df = pd.read_csv('dataset/train.csv', low_memory=False)\n",
    "df = df.drop('id', axis=1)\n",
    "# using the neural net's `cat`, `cont`, and `splits`\n",
    "procs = [Categorify]\n",
    "to = TabularPandas(df, procs, cat, cont, 'target', splits = splits)\n",
    "\n",
    "def rf(xs, y, n_estimators=40, max_samples=130_000,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf).fit(xs, y)\n",
    "\n",
    "def auc(m, xs, y):\n",
    "    preds = m.predict(xs)\n",
    "    return round(roc_auc_score(y, preds), 3)\n",
    "\n",
    "# Replacing Nominal variables with Embeddings\n",
    "learn = learn.load('learn8')\n",
    "\n",
    "def embed_features(learner, xs):\n",
    "    \"\"\"\n",
    "    learner: fastai Learner used to train the neural net\n",
    "    xs: DataFrame containing input variables with nominal values defined by their rank.\n",
    "    ::returns:: a copy of `xs` with embeddings replacing each categorical variable\n",
    "    \"\"\"\n",
    "    xs = xs.copy()\n",
    "    for i,col in enumerate(learn.dls.cat_names):\n",
    "        emb = learn.model.embeds[i]\n",
    "        emb_data = emb(tensor(xs[col], dtype=torch.int64).to(device))\n",
    "        emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n",
    "        feat_df = pd.DataFrame(data=emb_data, index=xs.index, columns=emb_names)\n",
    "        xs = xs.drop(col, axis=1)\n",
    "        xs = xs.join(feat_df)\n",
    "        return xs\n",
    "    \n",
    "emb_xs = embed_features(learn, to.train.xs)\n",
    "emb_valid_xs = embed_features(learn, to.valid.xs)\n",
    "emb_valid_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d69214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cat0_0</th>\n",
       "      <th>cat0_1</th>\n",
       "      <th>cat0_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155657</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297425</td>\n",
       "      <td>0.807528</td>\n",
       "      <td>0.618459</td>\n",
       "      <td>0.134682</td>\n",
       "      <td>0.294103</td>\n",
       "      <td>0.277333</td>\n",
       "      <td>0.328074</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173027</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611607</td>\n",
       "      <td>0.212078</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>0.564597</td>\n",
       "      <td>0.198777</td>\n",
       "      <td>0.244124</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604928</td>\n",
       "      <td>0.553467</td>\n",
       "      <td>0.619134</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.858435</td>\n",
       "      <td>0.764429</td>\n",
       "      <td>0.922626</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85105</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603042</td>\n",
       "      <td>0.555548</td>\n",
       "      <td>0.542915</td>\n",
       "      <td>0.292864</td>\n",
       "      <td>0.406706</td>\n",
       "      <td>0.368794</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239785</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474598</td>\n",
       "      <td>0.554855</td>\n",
       "      <td>0.341110</td>\n",
       "      <td>0.438878</td>\n",
       "      <td>0.412705</td>\n",
       "      <td>0.595286</td>\n",
       "      <td>0.329394</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156421</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572546</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.703250</td>\n",
       "      <td>0.788759</td>\n",
       "      <td>0.557523</td>\n",
       "      <td>0.525486</td>\n",
       "      <td>0.836878</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175912</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320616</td>\n",
       "      <td>0.419490</td>\n",
       "      <td>0.676380</td>\n",
       "      <td>0.400634</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>0.485152</td>\n",
       "      <td>0.451870</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108891</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287734</td>\n",
       "      <td>0.359661</td>\n",
       "      <td>0.511817</td>\n",
       "      <td>0.362875</td>\n",
       "      <td>0.329639</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>0.392249</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203945</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248299</td>\n",
       "      <td>0.734260</td>\n",
       "      <td>0.157658</td>\n",
       "      <td>0.325285</td>\n",
       "      <td>0.367817</td>\n",
       "      <td>0.280118</td>\n",
       "      <td>0.459436</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147299</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699901</td>\n",
       "      <td>0.682843</td>\n",
       "      <td>0.806265</td>\n",
       "      <td>0.379175</td>\n",
       "      <td>0.322530</td>\n",
       "      <td>0.268269</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>0.035863</td>\n",
       "      <td>0.053796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  ...  \\\n",
       "155657     2    19     2     5    34     1    36    55     1    149  ...   \n",
       "173027     6     1     2     5    34     1    47    10    11     70  ...   \n",
       "4182      12     9     2     6    34     1    40    61     1    250  ...   \n",
       "85105     13     1     1     6    34     1    40    24     5    160  ...   \n",
       "239785    11     1     2     6    34     1    12    39     6    211  ...   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   \n",
       "156421    11     1     1     6    34     1    40     4     3    178  ...   \n",
       "175912     8    16     1     6    34     1    47    51     1    175  ...   \n",
       "108891    12     3     2     5    34     1    15     5     1     76  ...   \n",
       "203945     9     4     1     5     3     3    19    60     1     70  ...   \n",
       "147299     6     3     6     6    34     1    17    40     1    122  ...   \n",
       "\n",
       "           cont4     cont5     cont6     cont7     cont8     cont9    cont10  \\\n",
       "155657  0.297425  0.807528  0.618459  0.134682  0.294103  0.277333  0.328074   \n",
       "173027  0.611607  0.212078  0.149391  0.564597  0.198777  0.244124  0.552505   \n",
       "4182    0.604928  0.553467  0.619134  0.601375  0.858435  0.764429  0.922626   \n",
       "85105   0.603042  0.555548  0.542915  0.292864  0.406706  0.368794  0.321800   \n",
       "239785  0.474598  0.554855  0.341110  0.438878  0.412705  0.595286  0.329394   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "156421  0.572546  0.737983  0.703250  0.788759  0.557523  0.525486  0.836878   \n",
       "175912  0.320616  0.419490  0.676380  0.400634  0.301467  0.485152  0.451870   \n",
       "108891  0.287734  0.359661  0.511817  0.362875  0.329639  0.307292  0.392249   \n",
       "203945  0.248299  0.734260  0.157658  0.325285  0.367817  0.280118  0.459436   \n",
       "147299  0.699901  0.682843  0.806265  0.379175  0.322530  0.268269  0.316153   \n",
       "\n",
       "          cat0_0    cat0_1    cat0_2  \n",
       "155657  0.055335  0.035863  0.053796  \n",
       "173027  0.055335  0.035863  0.053796  \n",
       "4182    0.055335  0.035863  0.053796  \n",
       "85105   0.055335  0.035863  0.053796  \n",
       "239785  0.055335  0.035863  0.053796  \n",
       "...          ...       ...       ...  \n",
       "156421  0.055335  0.035863  0.053796  \n",
       "175912  0.055335  0.035863  0.053796  \n",
       "108891  0.055335  0.035863  0.053796  \n",
       "203945  0.055335  0.035863  0.053796  \n",
       "147299  0.055335  0.035863  0.053796  \n",
       "\n",
       "[240000 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538f9288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_valid_xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899be3f",
   "metadata": {},
   "source": [
    "repeats = [  2,  2,  2,  2,  2,  2,  3,  4,  4, 4,  5]\n",
    "probas =  [.95, .4, .7, .9, .9, .9, .9, .9, .9, .9, .25]\n",
    "swap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n",
    "\n",
    "noise_maker = SwapNoiseMasker(swap_probas)\n",
    "print(f\"the probability vector is of size {sum(repeats)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcfad4",
   "metadata": {},
   "source": [
    "## 2. Test with only Kmeans \n",
    "* To do: To test a better algorithme like DCN, deep clusterNet\n",
    "* Peut être essayer de rajouter un kfold ? Car score à 0.9 pour AUC c'est top délire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00799a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Compute the clusters on the whole data\n",
    "# Then split it into batch\n",
    "\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- List of parameters --- #\n",
    "nb_clust = 50\n",
    "\n",
    "kmeans = KMeans(n_clusters=nb_clust, random_state=0)\n",
    "\n",
    "dict_clust = dict()\n",
    "\n",
    "# -- Clustering -- #\n",
    "# Retrieve the whole dataset\n",
    "train_loader, valid_loader = pandas_to_tensor(df, emb_xs, emb_valid_xs)\n",
    "\n",
    "l_tensor = []\n",
    "\n",
    "for batch, (x, _) in enumerate(train_loader):\n",
    "    l_tensor.append(x)\n",
    "    dict_clust[batch] = dict()\n",
    "    dict_clust[batch] = len(x)\n",
    "\n",
    "concat_tensors = torch.cat(l_tensor, dim=0)\n",
    "\n",
    "# Cluster the whole dataset\n",
    "clust_train = kmeans.fit_predict(concat_tensors)\n",
    "\n",
    "i = iter(clust_train)\n",
    "\n",
    "dico_cluster = {k: [next(i) for _ in range(v)] for k, v in dict_clust.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\"\n",
    "        To add dropout or other idk\n",
    "        input_size: df.shape[1]\n",
    "        output_size: nb_clust\n",
    "        \"\"\"\n",
    "        #self.fc1 = nn.Linear(in_features =input_size, out_features = 20)\n",
    "        self.fc1 = nn.Linear(in_features =input_size, out_features = output_size)\n",
    "\n",
    "        #self.fc2 = nn.Linear(in_features = 20, out_features=output_size)\n",
    "        self.fc3 = nn.Linear(in_features=output_size, out_features=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(num_features=output_size)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=output_size)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, n_clusters):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        #x = self.bn2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        if n_clusters == 2:\n",
    "            # binary classification\n",
    "            # returns a probability scalar\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "            \n",
    "        else:\n",
    "            # clustering\n",
    "            # return a vector of size number of clusters\n",
    "            #x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "\n",
    "train_loader, valid_loader = pandas_to_tensor(df, emb_xs, emb_valid_xs)\n",
    "\n",
    "#nb_clust = 15\n",
    "\n",
    "# -1 cause we remove the target\n",
    "net = Net(32, nb_clust)\n",
    "\n",
    "\n",
    "max_epochs = 2\n",
    "#loss_fct = nn.CrossEntropyLoss()\n",
    "loss_fct_binary = nn.BCEWithLogitsLoss()\n",
    "loss_fct_clustering = nn.CrossEntropyLoss()\n",
    "\n",
    "l_loss = list()\n",
    "l_loss_intermediate = list()\n",
    "l_loss_test = list()\n",
    "l_roc_train = list()\n",
    "l_roc_intermediary = list()\n",
    "l_roc_test = list()\n",
    "\n",
    "optim = opt.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    \"\"\"\n",
    "    Unsupervised: Pre-learning we learn with the pseudo labels from clustering\n",
    "    \"\"\"\n",
    "    t0 = datetime.now()\n",
    "    net.train()\n",
    "    for batch, (x, _) in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        #x, mask = noise_maker.apply(x)\n",
    "        # Predict soft-targets and embeddings\n",
    "        output = net(x, nb_clust)\n",
    "        \n",
    "        # negative log likelihood\n",
    "        m = nn.Softmax(dim=1)\n",
    "        pseudo_labels = np.array(dico_cluster[batch])\n",
    "        # note that in the loss I used to do m(output) that is re apply a softmax -> not necessary I think\n",
    "        loss = loss_fct_clustering(output, torch.tensor(pseudo_labels.astype(np.int8), dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        l_loss.append(loss.item())\n",
    "        #l_roc_train.append(roc_auc_score(y.detach().numpy(), proba.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad and 'fc1' in name:\n",
    "        param.requires_grad = False\n",
    "    elif param.requires_grad and 'fc2' in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optim = opt.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)\n",
    "\n",
    "max_epochs = 25\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    \"\"\"\n",
    "    Supervised: Second batch we learn to predict binary classification problem\n",
    "    \"\"\"\n",
    "    t0 = datetime.now()\n",
    "    net.train()\n",
    "    for batch, (x, y) in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        output = net(x, 2)\n",
    "        \n",
    "        loss = loss_fct_binary(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        l_loss_intermediate.append(loss.item())\n",
    "        l_roc_intermediary.append(roc_auc_score(y.detach().numpy(),  torch.sigmoid(output).detach().numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \"\"\"\n",
    "    Testing\n",
    "    \"\"\"\n",
    "    for batch, (x, y) in enumerate(valid_loader):\n",
    "        output = net(x, 2)\n",
    "\n",
    "        loss = loss_fct_binary(output, y)\n",
    "        l_loss_test.append(loss)\n",
    "        \n",
    "        l_roc_test.append(roc_auc_score(y.detach().numpy(), torch.sigmoid(output).detach().numpy()))\n",
    "        \n",
    "print_scores(l_loss, l_roc_train,l_loss_intermediate, l_roc_intermediary, l_roc_test, l_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d4715",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3a4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba95d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd210a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a00930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b7832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
