{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c4afa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.145474</td>\n",
       "      <td>0.131971</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119979</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115985</td>\n",
       "      <td>0.115925</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112822</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.111047</td>\n",
       "      <td>0.112324</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108471</td>\n",
       "      <td>0.111757</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.106970</td>\n",
       "      <td>0.110622</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.105566</td>\n",
       "      <td>0.110410</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cat0_0</th>\n",
       "      <th>cat0_1</th>\n",
       "      <th>cat0_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277491</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292775</td>\n",
       "      <td>0.210421</td>\n",
       "      <td>0.335680</td>\n",
       "      <td>0.508110</td>\n",
       "      <td>0.375804</td>\n",
       "      <td>0.485737</td>\n",
       "      <td>0.388535</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59826</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717059</td>\n",
       "      <td>0.555945</td>\n",
       "      <td>0.253756</td>\n",
       "      <td>0.810781</td>\n",
       "      <td>0.558822</td>\n",
       "      <td>0.821682</td>\n",
       "      <td>0.950494</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100532</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300573</td>\n",
       "      <td>0.807069</td>\n",
       "      <td>0.559432</td>\n",
       "      <td>0.310892</td>\n",
       "      <td>0.414348</td>\n",
       "      <td>0.328559</td>\n",
       "      <td>0.198210</td>\n",
       "      <td>-0.054680</td>\n",
       "      <td>-0.078122</td>\n",
       "      <td>-0.041497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127444</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260020</td>\n",
       "      <td>0.762579</td>\n",
       "      <td>0.929469</td>\n",
       "      <td>0.368868</td>\n",
       "      <td>0.400274</td>\n",
       "      <td>0.582639</td>\n",
       "      <td>0.436348</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268466</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.487171</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.332349</td>\n",
       "      <td>0.366024</td>\n",
       "      <td>0.374952</td>\n",
       "      <td>0.382322</td>\n",
       "      <td>-0.054680</td>\n",
       "      <td>-0.078122</td>\n",
       "      <td>-0.041497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292950</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310330</td>\n",
       "      <td>0.354413</td>\n",
       "      <td>0.335381</td>\n",
       "      <td>0.433126</td>\n",
       "      <td>0.758386</td>\n",
       "      <td>0.438863</td>\n",
       "      <td>0.488459</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235306</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731341</td>\n",
       "      <td>0.734535</td>\n",
       "      <td>0.548252</td>\n",
       "      <td>0.304629</td>\n",
       "      <td>0.338420</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.339444</td>\n",
       "      <td>-0.054680</td>\n",
       "      <td>-0.078122</td>\n",
       "      <td>-0.041497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187628</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278301</td>\n",
       "      <td>0.735457</td>\n",
       "      <td>0.787859</td>\n",
       "      <td>0.313780</td>\n",
       "      <td>0.700493</td>\n",
       "      <td>0.799430</td>\n",
       "      <td>0.464530</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200243</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247176</td>\n",
       "      <td>0.052474</td>\n",
       "      <td>0.172384</td>\n",
       "      <td>0.552959</td>\n",
       "      <td>0.405473</td>\n",
       "      <td>0.266106</td>\n",
       "      <td>0.531018</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155878</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305348</td>\n",
       "      <td>0.680329</td>\n",
       "      <td>0.534171</td>\n",
       "      <td>0.381683</td>\n",
       "      <td>0.435032</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.263918</td>\n",
       "      <td>0.056746</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10  ...  \\\n",
       "277491     6     4     2     7    34     1    31    60     1    148  ...   \n",
       "59826     11     1     1    13    34     3    20     4    13    258  ...   \n",
       "100532     6    10     1     5    34     3     1    36     1     76  ...   \n",
       "127444     8    13     1     6     3     1     9    60     1    148  ...   \n",
       "268466    13     1     2     5    34     1     9    52     5    160  ...   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   \n",
       "292950     8    13     1     6     3     5    47    51     1    175  ...   \n",
       "235306     6     3     1     5    34     3    20    11     1     55  ...   \n",
       "187628    15     3     1     6    34     1    37    34     1    148  ...   \n",
       "200243     6     1     2     5     3     1    10    47     5    175  ...   \n",
       "155878     6     4     1     6    34     3    23    37     1    119  ...   \n",
       "\n",
       "           cont4     cont5     cont6     cont7     cont8     cont9    cont10  \\\n",
       "277491  0.292775  0.210421  0.335680  0.508110  0.375804  0.485737  0.388535   \n",
       "59826   0.717059  0.555945  0.253756  0.810781  0.558822  0.821682  0.950494   \n",
       "100532  0.300573  0.807069  0.559432  0.310892  0.414348  0.328559  0.198210   \n",
       "127444  0.260020  0.762579  0.929469  0.368868  0.400274  0.582639  0.436348   \n",
       "268466  0.292645  0.487171  0.581556  0.332349  0.366024  0.374952  0.382322   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "292950  0.310330  0.354413  0.335381  0.433126  0.758386  0.438863  0.488459   \n",
       "235306  0.731341  0.734535  0.548252  0.304629  0.338420  0.321991  0.339444   \n",
       "187628  0.278301  0.735457  0.787859  0.313780  0.700493  0.799430  0.464530   \n",
       "200243  0.247176  0.052474  0.172384  0.552959  0.405473  0.266106  0.531018   \n",
       "155878  0.305348  0.680329  0.534171  0.381683  0.435032  0.490791  0.263918   \n",
       "\n",
       "          cat0_0    cat0_1    cat0_2  \n",
       "277491  0.056746  0.042776  0.048583  \n",
       "59826   0.056746  0.042776  0.048583  \n",
       "100532 -0.054680 -0.078122 -0.041497  \n",
       "127444  0.056746  0.042776  0.048583  \n",
       "268466 -0.054680 -0.078122 -0.041497  \n",
       "...          ...       ...       ...  \n",
       "292950  0.056746  0.042776  0.048583  \n",
       "235306 -0.054680 -0.078122 -0.041497  \n",
       "187628  0.056746  0.042776  0.048583  \n",
       "200243  0.056746  0.042776  0.048583  \n",
       "155878  0.056746  0.042776  0.048583  \n",
       "\n",
       "[60000 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.tabular.all import *\n",
    "from fastcore.utils import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_nn = pd.read_csv('train.csv', low_memory=False)\n",
    "df_nn_final = df_nn.drop('id', axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Categorical embedding\n",
    "\"\"\"\n",
    "\n",
    "cont,cat = cont_cat_split(df_nn_final, max_card=9000, dep_var='target')\n",
    "procs_nn = [Categorify, Normalize]\n",
    "splits = RandomSplitter(seed=23)(df_nn_final)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "to_nn = TabularPandas(df_nn, procs_nn, cat, cont,\n",
    "                      splits=splits, y_names='target')\n",
    "dls = to_nn.dataloaders(1024, device = device)\n",
    "\n",
    "learn = tabular_learner(dls, layers=[500,250], n_out=1)\n",
    "learn.fit_one_cycle(8, 5e-4)\n",
    "\n",
    "preds,targs = learn.get_preds()\n",
    "roc_auc_score(targs, preds)\n",
    "\n",
    "learn.save('learn8')\n",
    "\n",
    "# Machine Learning Models\n",
    "df = pd.read_csv('train.csv', low_memory=False)\n",
    "\n",
    "# using the neural net's `cat`, `cont`, and `splits`\n",
    "procs = [Categorify]\n",
    "to = TabularPandas(df, procs, cat, cont, 'target', splits = splits)\n",
    "\n",
    "def rf(xs, y, n_estimators=40, max_samples=130_000,\n",
    "       max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_samples=max_samples, max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf).fit(xs, y)\n",
    "\n",
    "def auc(m, xs, y):\n",
    "    preds = m.predict(xs)\n",
    "    return round(roc_auc_score(y, preds), 3)\n",
    "\n",
    "# Replacing Nominal variables with Embeddings\n",
    "learn = learn.load('learn8')\n",
    "\n",
    "def embed_features(learner, xs):\n",
    "    \"\"\"\n",
    "    learner: fastai Learner used to train the neural net\n",
    "    xs: DataFrame containing input variables with nominal values defined by their rank.\n",
    "    ::returns:: a copy of `xs` with embeddings replacing each categorical variable\n",
    "    \"\"\"\n",
    "    xs = xs.copy()\n",
    "    for i,col in enumerate(learn.dls.cat_names):\n",
    "        emb = learn.model.embeds[i]\n",
    "        emb_data = emb(tensor(xs[col], dtype=torch.int64).to(device))\n",
    "        emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n",
    "        feat_df = pd.DataFrame(data=emb_data, index=xs.index, columns=emb_names)\n",
    "        xs = xs.drop(col, axis=1)\n",
    "        xs = xs.join(feat_df)\n",
    "        return xs\n",
    "    \n",
    "emb_xs = embed_features(learn, to.train.xs)\n",
    "emb_valid_xs = embed_features(learn, to.valid.xs)\n",
    "emb_valid_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea731af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=32, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [05:38<00:00,  3.39s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "print_scores() missing 2 required positional arguments: 'l_roc_test' and 'l_loss_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fx/shjw_pvd7zv34xd99rfzhz2h0000gn/T/ipykernel_5616/4132259674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0ml_roc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mprint_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_roc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_roc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_loss_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: print_scores() missing 2 required positional arguments: 'l_roc_test' and 'l_loss_test'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *\n",
    "\n",
    "train_loader, valid_loader = pandas_to_tensor(df, emb_xs, emb_valid_xs)\n",
    "\n",
    "nb_clust = 1\n",
    "\n",
    "# -1 cause we remove the target\n",
    "net = Net(df.shape[1], nb_clust)\n",
    "print(net)\n",
    "\n",
    "max_epochs = 100\n",
    "#loss_fct = nn.CrossEntropyLoss()\n",
    "loss_fct = nn.BCEWithLogitsLoss()\n",
    "l_loss = list()\n",
    "l_loss_test = list()\n",
    "l_roc_train = list()\n",
    "l_roc_test = list()\n",
    "\n",
    "optim = opt.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    t0 = datetime.now()\n",
    "    net.train()\n",
    "    for batch, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Predict soft-targets and embeddings\n",
    "        proba, output = net(x)\n",
    "        \n",
    "        loss = loss_fct(proba, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        l_loss.append(loss.item())\n",
    "        l_roc_train.append(roc_auc_score(y.detach().numpy(), proba.detach().numpy()))\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(valid_loader):\n",
    "        proba, output = net(x)\n",
    "\n",
    "        loss = loss_fct(proba, y)\n",
    "        l_loss_test.append(loss)\n",
    "        \n",
    "        l_roc_test.append(roc_auc_score(y.detach().numpy(), proba.detach().numpy()))\n",
    "        \n",
    "print_scores(l_loss, l_roc_train, l_roc_test, l_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c46ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3789c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9ede5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fca0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
