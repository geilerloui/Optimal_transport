{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119da1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "logit_smote  = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=1000, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "logit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=1000, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "svc_lin  = SVC(C=1.0, class_weight=None, coef0=0.0,\n",
    "               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n",
    "               max_iter=1000, probability=True, random_state=None, shrinking=True,\n",
    "               tol=0.001, verbose=False)\n",
    "\n",
    "svc_rbf  = SVC(C=1.0, kernel='rbf', \n",
    "               degree= 3, gamma=1.0, \n",
    "               coef0=0.0, shrinking=True,\n",
    "               probability=True,tol=0.001,\n",
    "               class_weight=None,\n",
    "               verbose=False,max_iter= 1000,\n",
    "               random_state=None)\n",
    "\n",
    "xgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n",
    "                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n",
    "                    n_jobs=-1, objective='binary:logistic', random_state=0,\n",
    "                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "                    silent=True, subsample=1)\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "models = list()\n",
    "models.append(('LR', logit_smote))\n",
    "models.append(('SVM', svc_lin))\n",
    "models.append(('SVM-rbf', svc_rbf))\n",
    "models.append(('Gnb', gnb))\n",
    "models.append(('RF', rf))\n",
    "models.append(('DT', dtc))\n",
    "models.append(('XGBoost', xgc))\n",
    "models.append(('knn', knn))\n",
    "\n",
    "# Datasets\n",
    "dataset_name = ['Bank', 'C2C', 'DSN', 'HR', 'K2009', 'KKBox', 'Member', 'Mobile', 'SATO', 'TelC', 'TelE', 'UCI', 'news']\n",
    "dataset_name = ['Bank']\n",
    "\n",
    "# Sampling methods\n",
    "l_sampling = [\"plain\", \"smote\", \"adasyn\", \"tomekLinks\", \"ncr\", \"smoteRandom\", \"smoteTomek\", \"smoteNcr\"]\n",
    "l_sampling = [\"smote\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a798e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set for smote on fold 1 opened\n",
      "test set for smote on fold 1 opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:01:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold number 1\n",
      "training set for smote on fold 2 opened\n",
      "test set for smote on fold 2 opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:01:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold number 2\n",
      "training set for smote on fold 3 opened\n",
      "test set for smote on fold 3 opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:02:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:02:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold number 3\n",
      "training set for smote on fold 4 opened\n",
      "test set for smote on fold 4 opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:02:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:02:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold number 4\n",
      "training set for smote on fold 5 opened\n",
      "test set for smote on fold 5 opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/sklearn/svm/_base.py:258: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/louis_default/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:02:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:02:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fold number 5\n"
     ]
    }
   ],
   "source": [
    "for ds_name in dataset_name:\n",
    "\n",
    "    if ds_name == 'KKBox':\n",
    "        target_name = 'churn'\n",
    "        target_name_test = 'is_churn'\n",
    "    else:\n",
    "        target_name = 'churn'\n",
    "        target_name_test = 'churn'\n",
    "\n",
    "    fold_nb = 1\n",
    "    for sampling in l_sampling:\n",
    "        for fold_nb in range(1,6):\n",
    "            dico = {}\n",
    "            sep = \";\"\n",
    "\n",
    "            \n",
    "            path_train = f\"/home/ec2-user/SageMaker/data/churn_package/churn/sampled_datasets/{ds_name}/kfolds/{ds_name}_{sampling}_skf_k{fold_nb}_train.csv\"\n",
    "            path_test = f\"/home/ec2-user/SageMaker/data/churn_package/churn/sampled_datasets/{ds_name}/kfolds/{ds_name}_{sampling}_skf_k{fold_nb}_test.csv\"\n",
    "\n",
    "            telcom_train = pd.read_csv(path_train, sep)        \n",
    "            telcom_test = pd.read_csv(path_test, sep)  \n",
    "\n",
    "            X_train, y_train = telcom_train.drop(target_name, axis=1), telcom_train[target_name]\n",
    "            X_test, y_test = telcom_test.drop(target_name_test, axis=1), telcom_test[target_name_test]\n",
    "            #y_test.rename(columns={'is_churn': 'churn'}, inplace=True)\n",
    "\n",
    "            print(f\"training set for {sampling} on fold {fold_nb} opened\")\n",
    "            print(f\"test set for {sampling} on fold {fold_nb} opened\")\n",
    "\n",
    "            for name, model in models:\n",
    "                # begin\n",
    "                start = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                # return an array:\n",
    "                # first column = class 0, second column = class 1\n",
    "                # [0.5162321 , 0.4837679 ],\n",
    "                # [0.02658997, 0.97341003],\n",
    "                y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "                # end\n",
    "                end = time.time()\n",
    "                running_time = round((end - start), 5)  \n",
    "                    \n",
    "                features = X_train.columns                 \n",
    "\n",
    "                # for every single observation set name\n",
    "                dico[f\"name_{name}\"] = list(chain(*([n]*len(y_test) for n in [name])))\n",
    "                # \n",
    "                dico[f\"y_{name}\"] = y_test\n",
    "                dico[f\"yhat_{name}\"] = y_pred\n",
    "                dico[f\"p(y=1)_{name}\"] = np.round(y_pred_proba[:,1], decimals=4)\n",
    "                dico[f\"run_time_{name}\"] = list(chain(*([n]*len(y_test) for n in [running_time])))\n",
    "\n",
    "            path_out_test = f\"churn_package/churn/sampled_datasets/{ds_name}/results/{ds_name}_{sampling}_skf_k{fold_nb}.csv\"\n",
    "\n",
    "            #_write_dataframe_to_csv_on_s3(pd.DataFrame(dico), path_out_test)\n",
    "\n",
    "            print(f\"fold number {fold_nb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60bcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de580a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9748f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290f4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (louis_default)",
   "language": "python",
   "name": "louis_default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
